Running the StereoTopRGB Pipeline for Canopy Cover Data
--------------------------------------------------------

**Outline**

Welcome to PhytoOracle's StereoTop RGB pipeline! This pipeline uses the data transformers from the `AgPipeline group <https://github.com/AgPipeline/>`_ and the PhytoOracle group to extract canopy cover data from image files. 

**Transformers used**

StereoTopRGB currently uses 3 different transformers for data conversion:

.. list-table::
   :header-rows: 1
   
   * - Transformer
     - Process
   * - `cleanmetadata <https://github.com/AgPipeline/moving-transformer-cleanmetadata>`_
     - Cleans gantry generated metadata
   * - `bin2tif <https://github.com/AgPipeline/moving-transformer-bin2tif>`_
     - Converts bin compressed files to tif 
   * - `plotclip <https://github.com/AgPipeline/transformer-plotclip>`_ 
     - Clip GeoTIFF or LAS files according to plots


**Data overview**

PhytoOracle's StereoTop RGB requires a metadata file (:code:`<metadata>.json`) for every compressed image file (:code:`<image>.bin`). 
Each folder (one scan) contains one metadata file and 2 compressed images, one taken from a left camera and one taken from a right camera. These should already be in the same folder when obtaining data from the `CyVerse DataStore <https://cyverse.org/data-store>`_.

**Setup Guide**

Go `here <link>`_ to launch on an HPC system (tested on the University of Arizona's HPC system).

Go `here <link>`_ instead if using a cloud system with HPC support (tested on CyVerse Atmosphere, soon to be tested on NSF's JetStream).

**Running on the HPC's interactive node**

At this point your worker nodes should already be running and you should be in your FlirIr directory within your interactive node. Download the data that you need using:

.. code::

   iget -rKVP /iplant/home/shared/terraref/ua-mac/raw_tars/season_10_yr_2020/stereoTopRGB/<day>.tar


Replace :code:`<day>` with any day you want to process. Un-tar and move the folder to the stereoTopRGB directory.

.. code::

   tar -xvf <day>.tar
   mv ./stereoTopRGB/<day> ./

Then edit your :code:`entrypoint.sh` on line 4 to reflect the :code:`<day>` folder you want to process.

Once everything is edited, run the pipeline with :code:`./entrypoint.sh`.

**Running on the Cloud with HPC support**

Although very similar to the steps above, to run PhytoOracle on the Cloud with HPC support, there are a few extra steps  you have to carry out for data staging before starting the pipeline with :code:`./entrypoint.sh`.

Using your favouring editing tool do

.. code::

   /etc/nginx/sites-available/phyto_oracle.conf


paste the next snipped and save (changing the highlighted :code:`<fields>`)

.. code::

server {
        listen 80 default_server;
        listen [::]:80 default_server;

        # SSL configuration
        #
        # listen 443 ssl default_server;
        # listen [::]:443 ssl default_server;
        #
        # Note: You should disable gzip for SSL traffic.
        # See: https://bugs.debian.org/773332
        #
        # Read up on ssl_ciphers to ensure a secure configuration.
        # See: https://bugs.debian.org/765782
        #
        # Self signed certs generated by the ssl-cert package
        # Don't use them in a production server!
        #
        # include snippets/snakeoil.conf;

        root <PATH/TO/YOUR/PHYTOORACLE/PIPELINE>;

        index index.html index.htm index.nginx-debian.html;

        server_name _;

        location / {
                auth_basic "PhytoOracle Data";
        auth_basic_user_file /etc/apache2/.htpasswd;
                # First attempt to serve request as file, then
                # as directory, then fall back to displaying a 404.
                try_files $uri $uri/ =404;
                autoindex on;
        }
}


then from within your Transformer directory do :code:`./nginx_reload.sh`. Input your password if asked.

Open and edit :code:`process_one_set.sh` : 

- delete the :code:`#` on lines 44, 45, 46, 47
- remove :code:`${HPC_PATH}` on lines 23, 24, 25